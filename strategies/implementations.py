# strategies/implementations.py (Corrected and Refined)
import httpx
import xml.etree.ElementTree as ET
import json
import re
from bs4 import BeautifulSoup
from urllib.parse import quote_plus, urljoin

# å¯¼å…¥æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„æŠ½è±¡åŸºç±»
from strategies.download_strategy import DownloadStrategy


# --- ä¿ç•™çš„ä¸‹è½½å™¨ ---

class ArxivDownloader(DownloadStrategy):
    """ä»arXivä¸‹è½½è®ºæ–‡çš„ç­–ç•¥ã€‚"""

    def __init__(self, session: httpx.AsyncClient, save_dir: str):
        super().__init__(session, save_dir)
        self.api_url = "https://export.arxiv.org/api/query?"

    async def download(self, normalized_title: str, filepath: str) -> bool:
        print("   -> [Strategy: arXiv] Trying to find and download...")
        try:
            params = {"search_query": f'ti:"{normalized_title}"', "start": 0, "max_results": 1}
            response = await self.session.get(self.api_url, params=params)
            response.raise_for_status()
            root = ET.fromstring(response.content)
            namespace = {'atom': 'http://www.w3.org/2005/Atom'}
            entry = root.find('atom:entry', namespace)
            if entry:
                pdf_link_element = entry.find("atom:link[@title='pdf']", namespace)
                if pdf_link_element is not None and pdf_link_element.get('href'):
                    pdf_url = pdf_link_element.get('href')
                    return await self._download_pdf_from_url(pdf_url, filepath)
            print("   -> [Strategy: arXiv] ğŸŸ¡ Paper not found.")
            return False
        except Exception as e:
            print(f"   -> [Strategy: arXiv] âŒ An error occurred: {e}")
            return False


class CoreDownloader(DownloadStrategy):
    """
    ä»COREä¸‹è½½è®ºæ–‡çš„ç­–ç•¥ã€‚
    [ä¿®æ­£] å·²æ›´æ–°ä¸ºä½¿ç”¨POSTè¯·æ±‚å’ŒJSONè´Ÿè½½ï¼Œä»¥æé«˜ç¨³å®šæ€§ã€‚
    """

    def __init__(self, session: httpx.AsyncClient, save_dir: str, api_key: str):
        super().__init__(session, save_dir)
        self.api_url = "https://api.core.ac.uk/v3/search/works"
        self.api_key = api_key
        if self.api_key: self.headers["Authorization"] = f"Bearer {self.api_key}"

    async def download(self, normalized_title: str, filepath: str) -> bool:
        if not self.api_key: return False
        print("   -> [Strategy: CORE] Trying to find and download...")
        try:
            # ä½¿ç”¨POSTè¯·æ±‚å‘é€JSONæ•°æ®ï¼Œé¿å…URLç¼–ç é—®é¢˜
            data = {"q": f'title:("{normalized_title}")'}
            response = await self.session.post(self.api_url, json=data, headers=self.headers)
            response.raise_for_status()
            results = response.json()
            if results.get("results"):
                download_url = results["results"][0].get("downloadUrl")
                if download_url: return await self._download_pdf_from_url(download_url, filepath)
            print("   -> [Strategy: CORE] ğŸŸ¡ Paper not found or no download link.")
            return False
        except Exception as e:
            print(f"   -> [Strategy: CORE] âŒ An error occurred: {e}")
            return False


class AaaiOjsDownloader(DownloadStrategy):
    """ä» ojs.aaai.org ä¸‹è½½AAAIä¼šè®®è®ºæ–‡çš„ç­–ç•¥ã€‚"""

    def __init__(self, session: httpx.AsyncClient, save_dir: str):
        super().__init__(session, save_dir)
        self.search_url = "https://ojs.aaai.org/index.php/AAAI/search/search"

    async def download(self, normalized_title: str, filepath: str) -> bool:
        print("   -> [Strategy: AAAI OJS] Trying to find and download...")
        try:
            # ä½¿ç”¨æœªæ ‡å‡†åŒ–çš„æ ‡é¢˜è¿›è¡Œæœç´¢ï¼Œä»¥è·å¾—æ›´å¥½çš„åŒ¹é…æ•ˆæœ
            search_response = await self.session.get(self.search_url, params={'query': normalized_title})
            search_response.raise_for_status()
            soup = BeautifulSoup(search_response.text, 'html.parser')
            article_link = soup.select_one('h3.title a, h4.title a')
            if not article_link or not article_link.get('href'):
                print("   -> [Strategy: AAAI OJS] ğŸŸ¡ Paper not found.")
                return False
            article_page_url = article_link.get('href')
            article_response = await self.session.get(article_page_url)
            article_response.raise_for_status()
            article_soup = BeautifulSoup(article_response.text, 'html.parser')
            pdf_link = article_soup.select_one('a.obj_galley_link.pdf')
            if not pdf_link or not pdf_link.get('href'):
                print(f"   -> [Strategy: AAAI OJS] ğŸŸ¡ Found article page but no PDF link: {article_page_url}")
                return False
            pdf_url = pdf_link.get('href').replace('/view/', '/download/')
            return await self._download_pdf_from_url(pdf_url, filepath)
        except Exception as e:
            print(f"   -> [Strategy: AAAI OJS] âŒ An error occurred: {e}")
            return False


class NeuripsDownloader(DownloadStrategy):
    """
    ä» proceedings.neurips.cc ä¸‹è½½ NeurIPS è®ºæ–‡çš„ç­–ç•¥ã€‚
    [ä¿®æ­£] æ”¹è¿›äº†é“¾æ¥æŸ¥æ‰¾é€»è¾‘ï¼Œé¿å…è·³è½¬åˆ°adminç™»å½•é¡µã€‚
    """

    def __init__(self, session: httpx.AsyncClient, save_dir: str):
        super().__init__(session, save_dir)
        self.base_url = "https://proceedings.neurips.cc"
        self.search_url = f"{self.base_url}/papers/search"

    async def download(self, normalized_title: str, filepath: str) -> bool:
        print(f"   -> [Strategy: NeurIPS Search] Trying to find and download...")
        try:
            params = {'q': normalized_title}
            search_response = await self.session.get(self.search_url, params=params)
            search_response.raise_for_status()
            soup = BeautifulSoup(search_response.text, 'html.parser')

            # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡é“¾æ¥ï¼Œå¹¶ä¸æ ‡é¢˜è¿›è¡ŒåŒ¹é…
            paper_links = soup.select('div.container-fluid ul li a')
            found_link = None
            for link in paper_links:
                # è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™å’Œç©ºæ ¼çš„æ¨¡ç³ŠåŒ¹é…
                if normalized_title.lower().replace(" ", "") in link.get_text(strip=True).lower().replace(" ", ""):
                    found_link = link
                    break

            if not found_link or not found_link.get('href'):
                print("   -> [Strategy: NeurIPS Search] ğŸŸ¡ Paper not found in search results.")
                return False

            abstract_url = urljoin(self.base_url, found_link.get('href'))

            # ä»æ‘˜è¦é¡µé¢é“¾æ¥æ„å»ºPDFé“¾æ¥
            pdf_url = abstract_url.replace("Abstract.html", "Paper.pdf").replace("/hash/", "/file/")
            print(f"   -> [Strategy: NeurIPS Search] âœ… Found potential PDF link: {pdf_url}")
            return await self._download_pdf_from_url(pdf_url, filepath)

        except Exception as e:
            print(f"   -> [Strategy: NeurIPS Search] âŒ An error occurred: {e}")
            return False


# æ³¨æ„ï¼šACMå’ŒIEEEçš„httpxç‰ˆæœ¬å·²è¢«ç§»é™¤ï¼Œå› ä¸ºå®ƒä»¬ä¸å¯é ã€‚
# è¯·ä½¿ç”¨ä¸‹é¢æ–°æ–‡ä»¶ä¸­çš„Seleniumç‰ˆæœ¬ã€‚
class CvfDownloader(DownloadStrategy):
    """ä» CVF (openaccess.thecvf.com) ä¸‹è½½è®ºæ–‡ï¼Œä¾‹å¦‚ CVPR, ICCVã€‚"""

    def __init__(self, session: httpx.AsyncClient, save_dir: str):
        super().__init__(session, save_dir)
        self.base_url = "https://openaccess.thecvf.com"
        # CVFçš„æœç´¢åŠŸèƒ½æœ‰æ—¶ä¸ç¨³å®šï¼Œç›´æ¥è®¿é—®ä¼šè®®é¡µé¢å¯èƒ½æ›´å¥½
        # self.search_url = f"{self.base_url}/search_result"
        # æ­¤å¤„ä¿ç•™åŸæœ‰é€»è¾‘ï¼Œä½†å¯ä»¥è€ƒè™‘åç»­ä¼˜åŒ–ä¸ºç›´æ¥çˆ¬å–ä¼šè®®é¡µé¢

    async def download(self, normalized_title: str, filepath: str) -> bool:
        print("   -> [Strategy: CVF Open Access] Trying to find and download...")
        try:
            # 1. åœ¨CVFç½‘ç«™ä¸Šæœç´¢
            params = {"q": normalized_title}
            search_response = await self.session.get(f"{self.base_url}/search_result", params=params)
            search_response.raise_for_status()
            soup = BeautifulSoup(search_response.text, 'html.parser')

            # 2. æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ
            result_link = soup.select_one('div.content div dl dt a')
            if not result_link or not result_link.get('href'):
                print("   -> [Strategy: CVF Open Access] ğŸŸ¡ Paper not found in search results.")
                return False

            # 3. ä»æ‘˜è¦é¡µé¢é“¾æ¥æ„å»ºPDFé“¾æ¥
            abstract_url = urljoin(self.base_url, result_link.get('href'))

            # è®¿é—®æ‘˜è¦é¡µä»¥æ‰¾åˆ°PDFé“¾æ¥
            abstract_page_resp = await self.session.get(abstract_url)
            abstract_page_resp.raise_for_status()
            abstract_soup = BeautifulSoup(abstract_page_resp.text, 'html.parser')

            # å¯»æ‰¾åŒ…å« "pdf" æ–‡æœ¬çš„é“¾æ¥
            pdf_link = abstract_soup.find('a', href=re.compile(r'\.pdf$'))
            if not pdf_link:
                print(f"   -> [Strategy: CVF Open Access] ğŸŸ¡ Found abstract page but no PDF link: {abstract_url}")
                return False

            pdf_url = urljoin(abstract_url, pdf_link['href'])
            print(f"   -> [Strategy: CVF Open Access] âœ… Found PDF link: {pdf_url}")
            return await self._download_pdf_from_url(pdf_url, filepath)
        except Exception as e:
            print(f"   -> [Strategy: CVF Open Access] âŒ An error occurred: {e}")
            return False
